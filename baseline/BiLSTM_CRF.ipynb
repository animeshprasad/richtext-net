{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from data_reader import *\n",
    "from evaluate_new import *\n",
    "from nltk.tokenize import  SpaceTokenizer\n",
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model, Input\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn import metrics\n",
    "import re\n",
    "import os\n",
    "from keras_contrib.layers import CRF\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## prepare data for doc evaluation\n",
    "def get_doc_test(gold, text):\n",
    "    ## gold: gold data\n",
    "    ## text: full text file\n",
    "    test_labels = []\n",
    "    test_doc = []\n",
    "    with open(doc_dir+gold, 'r') as doc_labels, open(doc_dir+text, 'r') as doc_text:\n",
    "        d_labels = doc_labels.readlines()\n",
    "        d_text = doc_text.readlines()\n",
    "        assert len(d_labels) == len(d_text), \"Mismatch\"\n",
    "        for i in range(len(d_labels)):\n",
    "            ## label: start_id end_id data_id pub_id\n",
    "            test_labels.append(d_labels[i].strip())\n",
    "            \n",
    "            text = d_text[i].strip()\n",
    "            text = re.sub('\\d', '0', text)\n",
    "            text = re.sub('[^ ]- ', '', text)\n",
    "            \n",
    "            test_doc.append(text)\n",
    "    return test_labels, test_doc\n",
    "\n",
    "## convert one doc data to (text, label) format\n",
    "def read_doc(doc, labels):\n",
    "    doc = doc.strip().split()\n",
    "    labels = labels.strip().split('|')\n",
    "    labels = [la.split() for la in labels]\n",
    "    for i in range(len(labels)):\n",
    "        for j in range(len(labels[i])):\n",
    "            labels[i][j] = int(labels[i][j])\n",
    "\n",
    "    res_labels = [0]*len(doc)\n",
    "    for la in labels:\n",
    "        if la[2]!=0:\n",
    "            start = la[0]\n",
    "            end = la[1]\n",
    "            res_labels[start : end+1] = [1]*(end+1-start)\n",
    "    return [(doc[i], str(res_labels[i])) for i in range(len(doc))]\n",
    "\n",
    "## make prediction of one doc\n",
    "## split into segments first then combine results\n",
    "def doc_pred(model, doc, tokenizer, MAXLEN=40):\n",
    "    splits = []\n",
    "    for i in range(0, len(doc), MAXLEN):\n",
    "        splits.append(doc[i : i+MAXLEN])\n",
    "    splits = tokenizer.texts_to_sequences(splits)\n",
    "    splits = pad_sequences(splits, maxlen=MAXLEN)\n",
    "    preds = model.predict(splits)\n",
    "    preds = [np.argmax(y) for x in preds for y in x]\n",
    "    return preds\n",
    "\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [int(label) for token, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, label in sent]\n",
    "\n",
    "\n",
    "def prep_data(neg_ratio=0, val_ratio=0.05, data_dir='../../data/data_40/', maxlen=40, emb_dim=300):\n",
    "    train_sents, val_sents = data_sampler(neg_ratio, val_ratio, data_dir)\n",
    "\n",
    "    train_sents = get_sents(train_sents)\n",
    "    val_sents = get_sents(val_sents)\n",
    "\n",
    "    X_train = [sent2tokens(s) for s in train_sents]\n",
    "    Y_train = [sent2labels(s) for s in train_sents]\n",
    "\n",
    "    X_val = [sent2tokens(s) for s in val_sents]\n",
    "    Y_val = [sent2labels(s) for s in val_sents]\n",
    "    \n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "    word_index = tokenizer.word_index\n",
    "\n",
    "    vocab_size = len(word_index)+1\n",
    "    print (\"Vocab size: \", vocab_size)\n",
    "\n",
    "    all_embs = np.stack(embedding_index.values())\n",
    "    emb_mean, emb_std = np.mean(all_embs), np.std(all_embs)\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (vocab_size, emb_dim))\n",
    "    counter = 0\n",
    "    # embedding_matrix = np.zeros((vocab_size, emb_dim))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            counter += 1\n",
    "        else:\n",
    "            embedding_matrix[i] = np.random.randn(emb_dim)\n",
    "    print (\"{}/{} words covered in glove\".format(counter, vocab_size))\n",
    "\n",
    "    X_train = tokenizer.texts_to_sequences(X_train)\n",
    "    X_val = tokenizer.texts_to_sequences(X_val)\n",
    "\n",
    "    X_train = pad_sequences(X_train, maxlen=maxlen)\n",
    "    X_val = pad_sequences(X_val, maxlen=maxlen)\n",
    "\n",
    "    Y_train = np.asarray(Y_train)\n",
    "    Y_val = np.asarray(Y_val)\n",
    "\n",
    "    #labels need to be 3D\n",
    "    Y_train = np.expand_dims(Y_train, axis=2)\n",
    "    Y_val = np.expand_dims(Y_val, axis=2)\n",
    "\n",
    "    return X_train, Y_train, X_val, Y_val, embedding_matrix, vocab_size, tokenizer\n",
    "\n",
    "\n",
    "def run(X_train, Y_train, X_val, Y_val, embedding_matrix, vocab_size, maxlen=40, emb_dim=300, neg_ratio=0, hidden_dim=300, drop=0.2, r_drop=0.1):\n",
    "    ##build model\n",
    "#     input = Input(shape=(maxlen,))\n",
    "#     model = Embedding(vocab_size, emb_dim, weights=[embedding_matrix], input_length=maxlen, trainable=False)(input)\n",
    "#     model = Dropout(drop)(model)\n",
    "#     model = Bidirectional(LSTM(hidden_dim, return_sequences=True, recurrent_dropout=r_drop))(model)\n",
    "#     model = Dropout(drop)(model)\n",
    "#     out = TimeDistributed(Dense(1, activation='sigmoid'))(model)\n",
    "    input = Input(shape=(maxlen,))\n",
    "    model = Embedding(vocab_size, emb_dim, weights=[embedding_matrix], input_length=maxlen, trainable=False)(input)\n",
    "    model = Bidirectional(LSTM(hidden_dim, return_sequences=True, recurrent_dropout=r_drop))(model)\n",
    "    model = TimeDistributed(Dense(hidden_dim//4, activation='relu'))(model)\n",
    "    model = TimeDistributed(Dropout(drop))(model)\n",
    "    ##use CRF instead of Dense\n",
    "    crf = CRF(2)\n",
    "    out = crf(model)\n",
    "\n",
    "    model = Model(input, out)\n",
    "    \n",
    "    Y_train_2 = keras.utils.to_categorical(Y_train)\n",
    "    Y_val_2 = keras.utils.to_categorical(Y_val)\n",
    "    \n",
    "    model.compile(optimizer='adam', loss=crf.loss_function, metrics=[crf.accuracy]) \n",
    "    earlyStop = [EarlyStopping(monitor='val_loss', patience=1)]\n",
    "    history = model.fit(X_train, Y_train_2, batch_size=64, epochs=10, \n",
    "                       validation_data=(X_val, Y_val_2), callbacks=earlyStop)\n",
    "\n",
    "\n",
    "    preds = model.predict(X_val)\n",
    "    test = [[np.argmax(y) for y in x] for x in preds]\n",
    "    test_arr = np.asarray(test)\n",
    "    test_arr = np.reshape(test_arr, (-1))\n",
    "\n",
    "    print (metrics.precision_recall_fscore_support(np.reshape(Y_val,(-1)), test_arr, average=None,\n",
    "                                              labels=[0, 1]))\n",
    "\n",
    "    \n",
    "#     Y_pred_ = [[1 if y>=threshold else 0 for y in x] for x in Y_pred]\n",
    "    Y_val_ = np.squeeze(Y_val)\n",
    "\n",
    "    print (\"Evaluate: dev seg exact\")\n",
    "    pred_out_dir = out_dir+'seg_'+str(neg_ratio)+'neg'\n",
    "    gold_dir = '../../data/val_segs/'+'seg_'+str(neg_ratio)+'neg'\n",
    "    p, r, f = seg_exact_match(test, Y_val_, pred_out_dir, gold_dir)\n",
    "    \n",
    "    return model, history, p, r, f\n",
    "\n",
    "\n",
    "def doc_eval(model, tokenizer, doc_test, doc_out_dir, gold_dir, MAXLEN=40):\n",
    "    doc_preds = [doc_pred(model, d, tokenizer, MAXLEN) for d in doc_test]\n",
    "#     doc_preds = [[np.argmax(y) for y in x] for x in doc_preds]\n",
    "#     doc_preds = [[int(a) for a in x] for x in doc_preds]\n",
    "\n",
    "    with open(doc_out_dir, 'w') as fout:\n",
    "        for i in range(len(doc_preds)):\n",
    "            first = 0\n",
    "            j = 0\n",
    "            string = ''\n",
    "            no_mention = True\n",
    "            while j<len(doc_preds[i]):\n",
    "                while j<len(doc_preds[i]) and doc_preds[i][j]== 0:\n",
    "                    j+=1\n",
    "                if j<len(doc_preds[i]) and doc_preds[i][j] == 1:\n",
    "                    no_mention=False\n",
    "                    start = j\n",
    "                    while j+1<len(doc_preds[i]) and doc_preds[i][j+1]==1:\n",
    "                        j+=1\n",
    "                    end = j \n",
    "                    if first > 0:\n",
    "                        string += \" | \"\n",
    "                    string += (str(start)+' '+str(end))\n",
    "                    j+=1\n",
    "                    first += 1\n",
    "            if no_mention:\n",
    "                fout.write(\"-1 -1\"+'\\n')\n",
    "            else:\n",
    "                fout.write(string+'\\n')\n",
    "    print ('evaluating data from: ', doc_out_dir)\n",
    "    print ('doc exact: ', doc_exact_match(doc_out_dir, gold_dir))\n",
    "    print ('doc partial: ', doc_partial_match(doc_out_dir, gold_dir))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##load glove\n",
    "embedding_index = {}\n",
    "f = open('../../glove.6B.300d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embedding_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "\n",
    "threshold = 0.4\n",
    "doc_dir = \"../../data/all_test_docs/\"\n",
    "doc_test_y, doc_test_x = get_doc_test('test_doc_gold', 'test_docs')\n",
    "doc_tests = [read_doc(doc_test_x[d], doc_test_y[d]) for d in range(len(doc_test_x))]\n",
    "doc_tests = [sent2tokens(s) for s in doc_tests]\n",
    "\n",
    "zero_shot_y, zero_shot_x = get_doc_test('zero_shot_doc_gold', 'zero_shot_docs')\n",
    "zero_shot_tests = [read_doc(zero_shot_x[d], zero_shot_y[d]) for d in range(len(zero_shot_x))]\n",
    "zero_shot_tests = [sent2tokens(s) for s in zero_shot_tests]\n",
    "\n",
    "MAXLEN = 40\n",
    "DIR = '../../data/data_40/'\n",
    "out_dir = '../../outputs/'\n",
    "\n",
    "if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26407 pos data sampled\n",
      "11589 neg data sampled\n",
      "Vocab size:  68498\n",
      "19964/68498 words covered in glove\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train, X_val, Y_val, embedding_matrix, vocab_size, tokenizer = prep_data(neg_ratio=neg_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36096 samples, validate on 1900 samples\n",
      "Epoch 1/10\n",
      "36096/36096 [==============================] - 119s 3ms/step - loss: 0.0515 - acc: 0.9711 - val_loss: 0.0467 - val_acc: 0.9754\n",
      "Epoch 2/10\n",
      "36096/36096 [==============================] - 113s 3ms/step - loss: 0.0180 - acc: 0.9828 - val_loss: 0.0390 - val_acc: 0.9774\n",
      "Epoch 3/10\n",
      "36096/36096 [==============================] - 110s 3ms/step - loss: 0.0025 - acc: 0.9849 - val_loss: 0.0238 - val_acc: 0.9776\n",
      "Epoch 4/10\n",
      "36096/36096 [==============================] - 109s 3ms/step - loss: -0.0116 - acc: 0.9861 - val_loss: 0.0156 - val_acc: 0.9772\n",
      "Epoch 5/10\n",
      "36096/36096 [==============================] - 108s 3ms/step - loss: -0.0257 - acc: 0.9867 - val_loss: 0.0049 - val_acc: 0.9765\n",
      "Epoch 6/10\n",
      "36096/36096 [==============================] - 108s 3ms/step - loss: -0.0401 - acc: 0.9870 - val_loss: -0.0069 - val_acc: 0.9769\n",
      "Epoch 7/10\n",
      "36096/36096 [==============================] - 109s 3ms/step - loss: -0.0545 - acc: 0.9872 - val_loss: -0.0154 - val_acc: 0.9769\n",
      "Epoch 8/10\n",
      "36096/36096 [==============================] - 108s 3ms/step - loss: -0.0693 - acc: 0.9875 - val_loss: -0.0236 - val_acc: 0.9769\n",
      "Epoch 9/10\n",
      "36096/36096 [==============================] - 108s 3ms/step - loss: -0.0840 - acc: 0.9876 - val_loss: -0.0351 - val_acc: 0.9763\n",
      "Epoch 10/10\n",
      "36096/36096 [==============================] - 108s 3ms/step - loss: -0.0987 - acc: 0.9878 - val_loss: -0.0451 - val_acc: 0.9774\n",
      "(array([0.98627708, 0.7358841 ]), array([0.99026201, 0.66320723]), array([0.98826553, 0.69765804]), array([73013,  2987]))\n",
      "Evaluate: dev seg exact\n",
      "Doc exact: \n",
      " precision: 0.28274428274428276, recall: 0.2863157894736842, f1: 0.28451882845188287\n",
      "evaluating data from:  ../../outputs/doc_40_0.025neg\n",
      "Doc exact: \n",
      " precision: 0.14947582008792695, recall: 0.15640481245576787, f1: 0.1528618364170846\n",
      "doc exact:  (0.14947582008792695, 0.15640481245576787, 0.1528618364170846)\n",
      "Doc Token wise: \n",
      " precision: 0.3173618940248027, recall: 0.3612447866538338, f1: 0.3378844711177794\n",
      "doc partial:  (0.3173618940248027, 0.3612447866538338, 0.3378844711177794)\n",
      "evaluating data from:  ../../outputs/zeroshot_40_0.025neg\n",
      "Doc exact: \n",
      " precision: 0.09991235758106924, recall: 0.09819121447028424, f1: 0.09904430929626412\n",
      "doc exact:  (0.09991235758106924, 0.09819121447028424, 0.09904430929626412)\n",
      "Doc Token wise: \n",
      " precision: 0.2714072970960536, recall: 0.24940130003421143, f1: 0.2599393831342485\n",
      "doc partial:  (0.2714072970960536, 0.24940130003421143, 0.2599393831342485)\n"
     ]
    }
   ],
   "source": [
    "neg_ratio = 0.025\n",
    "hidden_dim = 100\n",
    "drop = 0.1\n",
    "r_drop=0.1\n",
    "model, history, p, r, f = run(X_train, Y_train, X_val, Y_val, embedding_matrix, vocab_size, neg_ratio=neg_ratio, hidden_dim=hidden_dim, drop=drop, r_drop=r_drop)\n",
    "doc_eval(model, tokenizer, doc_tests, out_dir+'doc_40_'+str(neg_ratio)+'neg', '../../data/all_test_docs/test_doc_gold')\n",
    "doc_eval(model, tokenizer, zero_shot_tests, out_dir+'zeroshot_40_'+str(neg_ratio)+'neg', '../../data/all_test_docs/zero_shot_doc_gold')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36096 samples, validate on 1900 samples\n",
      "Epoch 1/10\n",
      "36096/36096 [==============================] - 217s 6ms/step - loss: 0.0670 - acc: 0.9685 - val_loss: 0.0400 - val_acc: 0.9759\n",
      "Epoch 2/10\n",
      "36096/36096 [==============================] - 213s 6ms/step - loss: 0.0157 - acc: 0.9830 - val_loss: 0.0308 - val_acc: 0.9770\n",
      "Epoch 3/10\n",
      "36096/36096 [==============================] - 213s 6ms/step - loss: 4.7699e-04 - acc: 0.9851 - val_loss: 0.0233 - val_acc: 0.9768\n",
      "Epoch 4/10\n",
      "36096/36096 [==============================] - 213s 6ms/step - loss: -0.0127 - acc: 0.9861 - val_loss: 0.0155 - val_acc: 0.9772\n",
      "Epoch 5/10\n",
      "36096/36096 [==============================] - 212s 6ms/step - loss: -0.0262 - acc: 0.9867 - val_loss: 0.0052 - val_acc: 0.9769\n",
      "Epoch 6/10\n",
      "36096/36096 [==============================] - 211s 6ms/step - loss: -0.0398 - acc: 0.9870 - val_loss: -0.0043 - val_acc: 0.9771\n",
      "Epoch 7/10\n",
      "36096/36096 [==============================] - 211s 6ms/step - loss: -0.0545 - acc: 0.9875 - val_loss: -0.0126 - val_acc: 0.9777\n",
      "Epoch 8/10\n",
      "36096/36096 [==============================] - 211s 6ms/step - loss: -0.0690 - acc: 0.9876 - val_loss: -0.0196 - val_acc: 0.9774\n",
      "Epoch 9/10\n",
      "36096/36096 [==============================] - 211s 6ms/step - loss: -0.0837 - acc: 0.9878 - val_loss: -0.0218 - val_acc: 0.9774\n",
      "Epoch 10/10\n",
      "36096/36096 [==============================] - 211s 6ms/step - loss: -0.0986 - acc: 0.9880 - val_loss: -0.0294 - val_acc: 0.9775\n",
      "(array([0.98703552, 0.72785714]), array([0.9895635 , 0.68228992]), array([0.98829789, 0.70433731]), array([73013,  2987]))\n",
      "Evaluate: dev seg exact\n",
      "Doc exact: \n",
      " precision: 0.2866805411030177, recall: 0.29, f1: 0.28833071690214546\n",
      "evaluating data from:  ../../outputs/doc_40_0.025neg\n",
      "Doc exact: \n",
      " precision: 0.15427967594223319, recall: 0.15498938428874734, f1: 0.1546337157987643\n",
      "doc exact:  (0.15427967594223319, 0.15498938428874734, 0.1546337157987643)\n",
      "Doc Token wise: \n",
      " precision: 0.34684003628666465, recall: 0.3679820340070581, f1: 0.35709838107098385\n",
      "doc partial:  (0.34684003628666465, 0.3679820340070581, 0.35709838107098385)\n",
      "evaluating data from:  ../../outputs/zeroshot_40_0.025neg\n",
      "Doc exact: \n",
      " precision: 0.10623781676413255, recall: 0.09388458225667529, f1: 0.09967992684042067\n",
      "doc exact:  (0.10623781676413255, 0.09388458225667529, 0.09967992684042067)\n",
      "Doc Token wise: \n",
      " precision: 0.29318709133094284, recall: 0.23776941498460485, f1: 0.26258619061112687\n",
      "doc partial:  (0.29318709133094284, 0.23776941498460485, 0.26258619061112687)\n"
     ]
    }
   ],
   "source": [
    "neg_ratio = 0.025\n",
    "hidden_dim = 200\n",
    "drop = 0.2\n",
    "r_drop=0.2\n",
    "model, history, p, r, f = run(X_train, Y_train, X_val, Y_val, embedding_matrix, vocab_size, neg_ratio=neg_ratio, hidden_dim=hidden_dim, drop=drop, r_drop=r_drop)\n",
    "doc_eval(model, tokenizer, doc_tests, out_dir+'doc_40_'+str(neg_ratio)+'neg', '../../data/all_test_docs/test_doc_gold')\n",
    "doc_eval(model, tokenizer, zero_shot_tests, out_dir+'zeroshot_40_'+str(neg_ratio)+'neg', '../../data/all_test_docs/zero_shot_doc_gold')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36096 samples, validate on 1900 samples\n",
      "Epoch 1/10\n",
      "36096/36096 [==============================] - 134s 4ms/step - loss: 0.0672 - acc: 0.9787 - val_loss: 0.0666 - val_acc: 0.9767\n",
      "Epoch 2/10\n",
      "36096/36096 [==============================] - 125s 3ms/step - loss: 0.0345 - acc: 0.9845 - val_loss: 0.0532 - val_acc: 0.9779\n",
      "Epoch 3/10\n",
      "36096/36096 [==============================] - 124s 3ms/step - loss: 0.0151 - acc: 0.9859 - val_loss: 0.0413 - val_acc: 0.9777\n",
      "Epoch 4/10\n",
      "36096/36096 [==============================] - 124s 3ms/step - loss: -0.0017 - acc: 0.9865 - val_loss: 0.0335 - val_acc: 0.9773\n",
      "Epoch 5/10\n",
      "36096/36096 [==============================] - 125s 3ms/step - loss: -0.0175 - acc: 0.9871 - val_loss: 0.0189 - val_acc: 0.9769\n",
      "Epoch 6/10\n",
      "36096/36096 [==============================] - 125s 3ms/step - loss: -0.0330 - acc: 0.9874 - val_loss: 0.0127 - val_acc: 0.9766\n",
      "Epoch 7/10\n",
      "36096/36096 [==============================] - 125s 3ms/step - loss: -0.0482 - acc: 0.9877 - val_loss: 0.0079 - val_acc: 0.9779\n",
      "Epoch 8/10\n",
      "36096/36096 [==============================] - 124s 3ms/step - loss: -0.0631 - acc: 0.9879 - val_loss: 0.0014 - val_acc: 0.9777\n",
      "Epoch 9/10\n",
      "36096/36096 [==============================] - 125s 3ms/step - loss: -0.0781 - acc: 0.9880 - val_loss: -0.0108 - val_acc: 0.9764\n",
      "Epoch 10/10\n",
      "36096/36096 [==============================] - 125s 3ms/step - loss: -0.0929 - acc: 0.9882 - val_loss: -0.0102 - val_acc: 0.9767\n",
      "(array([0.98699825, 0.71288515]), array([0.98876912, 0.68162035]), array([0.98788289, 0.69690228]), array([73013,  2987]))\n",
      "Evaluate: dev seg exact\n",
      "Doc exact: \n",
      " precision: 0.2799586776859504, recall: 0.28526315789473683, f1: 0.28258602711157454\n",
      "evaluating data from:  ../../outputs/doc_40_0.025neg\n",
      "Doc exact: \n",
      " precision: 0.1270687237026648, recall: 0.1602972399150743, f1: 0.14176185260522609\n",
      "doc exact:  (0.1270687237026648, 0.1602972399150743, 0.14176185260522609)\n",
      "Doc Token wise: \n",
      " precision: 0.2971547706647044, recall: 0.38867500802053256, f1: 0.33680845148735056\n",
      "doc partial:  (0.2971547706647044, 0.38867500802053256, 0.33680845148735056)\n",
      "evaluating data from:  ../../outputs/zeroshot_40_0.025neg\n",
      "Doc exact: \n",
      " precision: 0.07758053911900066, recall: 0.10163652024117141, f1: 0.08799403430275914\n",
      "doc exact:  (0.07758053911900066, 0.10163652024117141, 0.08799403430275914)\n",
      "Doc Token wise: \n",
      " precision: 0.22355733570686842, recall: 0.25778309955525147, f1: 0.23945340430603004\n",
      "doc partial:  (0.22355733570686842, 0.25778309955525147, 0.23945340430603004)\n"
     ]
    }
   ],
   "source": [
    "neg_ratio = 0.025\n",
    "hidden_dim = 120\n",
    "drop = 0.2\n",
    "r_drop=0.2\n",
    "model, history, p, r, f = run(X_train, Y_train, X_val, Y_val, embedding_matrix, vocab_size, neg_ratio=neg_ratio, hidden_dim=hidden_dim, drop=drop, r_drop=r_drop)\n",
    "doc_eval(model, tokenizer, doc_tests, out_dir+'doc_40_'+str(neg_ratio)+'neg', '../../data/all_test_docs/test_doc_gold')\n",
    "doc_eval(model, tokenizer, zero_shot_tests, out_dir+'zeroshot_40_'+str(neg_ratio)+'neg', '../../data/all_test_docs/zero_shot_doc_gold')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36096 samples, validate on 1900 samples\n",
      "Epoch 1/10\n",
      "36096/36096 [==============================] - 412s 11ms/step - loss: 0.0529 - acc: 0.9792 - val_loss: 0.0543 - val_acc: 0.9779\n",
      "Epoch 2/10\n",
      "36096/36096 [==============================] - 1445s 40ms/step - loss: 0.0224 - acc: 0.9845 - val_loss: 0.0436 - val_acc: 0.9779\n",
      "Epoch 3/10\n",
      "22976/36096 [==================>...........] - ETA: 2:12 - loss: 0.0054 - acc: 0.9863"
     ]
    }
   ],
   "source": [
    "neg_ratio = 0.025\n",
    "hidden_dim = 300\n",
    "drop = 0.3\n",
    "r_drop=0.2\n",
    "model, history, p, r, f = run(X_train, Y_train, X_val, Y_val, embedding_matrix, vocab_size, neg_ratio=neg_ratio, hidden_dim=hidden_dim, drop=drop, r_drop=r_drop)\n",
    "doc_eval(model, tokenizer, doc_tests, out_dir+'doc_40_'+str(neg_ratio)+'neg', '../../data/all_test_docs/test_doc_gold')\n",
    "doc_eval(model, tokenizer, zero_shot_tests, out_dir+'zeroshot_40_'+str(neg_ratio)+'neg', '../../data/all_test_docs/zero_shot_doc_gold')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
