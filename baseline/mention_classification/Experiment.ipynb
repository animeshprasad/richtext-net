{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31226/50296 words covered in glove\n",
      "Train on 30208 samples, validate on 3356 samples\n",
      "Epoch 1/10\n",
      "30208/30208 [==============================] - 574s 19ms/step - loss: 5.9994 - acc: 0.0720 - val_loss: 5.4482 - val_acc: 0.0343\n",
      "Epoch 2/10\n",
      "30208/30208 [==============================] - 506s 17ms/step - loss: 5.1851 - acc: 0.0360 - val_loss: 4.8652 - val_acc: 0.0343\n",
      "Epoch 3/10\n",
      "30208/30208 [==============================] - 311s 10ms/step - loss: 4.6464 - acc: 0.0478 - val_loss: 4.2830 - val_acc: 0.0876\n",
      "Epoch 4/10\n",
      "30208/30208 [==============================] - 286s 9ms/step - loss: 4.0956 - acc: 0.1282 - val_loss: 3.9040 - val_acc: 0.1549\n",
      "Epoch 5/10\n",
      "30208/30208 [==============================] - 295s 10ms/step - loss: 3.7363 - acc: 0.1573 - val_loss: 3.5817 - val_acc: 0.1698\n",
      "Epoch 6/10\n",
      "30208/30208 [==============================] - 305s 10ms/step - loss: 3.4603 - acc: 0.1785 - val_loss: 3.3922 - val_acc: 0.1880\n",
      "Epoch 7/10\n",
      "30208/30208 [==============================] - 301s 10ms/step - loss: 3.2514 - acc: 0.2082 - val_loss: 3.2723 - val_acc: 0.2321\n",
      "Epoch 8/10\n",
      "30208/30208 [==============================] - 312s 10ms/step - loss: 3.0710 - acc: 0.2346 - val_loss: 3.1286 - val_acc: 0.2461\n",
      "Epoch 9/10\n",
      "30208/30208 [==============================] - 291s 10ms/step - loss: 2.9154 - acc: 0.2504 - val_loss: 3.0485 - val_acc: 0.2503\n",
      "Epoch 10/10\n",
      "30208/30208 [==============================] - 298s 10ms/step - loss: 2.7905 - acc: 0.2621 - val_loss: 2.9967 - val_acc: 0.2506\n"
     ]
    }
   ],
   "source": [
    "import keras \n",
    "from keras import layers\n",
    "from keras.layers import Embedding, Dropout, Dense, LSTM, Bidirectional, Input, Dense, Flatten\n",
    "from keras.models import Sequential, Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import numpy as np \n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import json\n",
    "import pandas as pd \n",
    "import os\n",
    "\n",
    "def data_loader(file):\n",
    "    with open(file, 'r') as f:\n",
    "        sents = []\n",
    "        labels = []\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            labels.append(int(line[2]))\n",
    "            sents.append(' '.join(line[4:]))\n",
    "    return sents, labels\n",
    "\n",
    "\n",
    "train_sents, train_labels = data_loader(\"../../data/train.txt\")\n",
    "val_sents, val_labels = data_loader(\"../../data/validate.txt\")\n",
    "test_sents, test_labels = data_loader(\"../../data/test.txt\")\n",
    "\n",
    "\n",
    "data_set = pd.read_json('../../../train_test/data_sets.json', encoding='utf-8')\n",
    "#note: dataset_id = index + 1\n",
    "data_description = data_set[\"description\"].values\n",
    "\n",
    "\n",
    "# Add a sentence for no mention case\n",
    "data_description = list(data_description)\n",
    "data_description.insert(0, \"There is no mention.\")\n",
    "\n",
    "\n",
    "maxlen = 100\n",
    "#vocab_size = 40000 ##more than 80K unique tokens\n",
    "emb_dim = 50\n",
    "HIDDEN_DIM = 256\n",
    "EPOCHS = 10  ## train more epochs with GPU, it takes 1h per epoch on my CPU\n",
    "NEG_RATIO = 3\n",
    "BATCH_SIZE = 10\n",
    "DATASET_CLASS = len(data_description) \n",
    "MODEL_NAME = \"LSTM\"\n",
    "\n",
    "#actual batch size = BATCH_SIZE * (1 + NEG_RATIO)\n",
    "\n",
    "##load glove\n",
    "embedding_index = {}\n",
    "f = open('../glove.6B.50d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embedding_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "###NOT using dataset info anymore\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_sents+val_sents+test_sents)\n",
    "X_train = tokenizer.texts_to_sequences(train_sents)\n",
    "X_val = tokenizer.texts_to_sequences(val_sents)\n",
    "X_test = tokenizer.texts_to_sequences(test_sents)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index)+1\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, emb_dim))\n",
    "counter = 0\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        counter += 1\n",
    "    else:\n",
    "        embedding_matrix[i] = np.random.randn(emb_dim)\n",
    "print (\"{}/{} words covered in glove\".format(counter, vocab_size))\n",
    "\n",
    "X_train = pad_sequences(X_train, maxlen=maxlen)\n",
    "X_val = pad_sequences(X_val, maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, maxlen=maxlen)\n",
    "Y_train = np.asarray(train_labels)\n",
    "Y_val = np.asarray(val_labels)\n",
    "Y_test = np.asarray(test_labels)\n",
    "Y_train = keras.utils.to_categorical(Y_train, num_classes=DATASET_CLASS)\n",
    "Y_val = keras.utils.to_categorical(Y_val, num_classes=DATASET_CLASS)\n",
    "Y_test = keras.utils.to_categorical(Y_test, num_classes=DATASET_CLASS)\n",
    "\n",
    "\n",
    "##randomly shuffle data and labels\n",
    "##np.random.seed(0)\n",
    "N = X_train.shape[0]\n",
    "indices = np.arange(N)\n",
    "np.random.shuffle(indices)\n",
    "X_train = X_train[indices]\n",
    "Y_train = Y_train[indices] \n",
    "\n",
    "\n",
    "def build_model():\n",
    "    embedding_layer = Embedding(vocab_size, emb_dim, weights=[embedding_matrix], input_length=maxlen, trainable=False)\n",
    "    article_input = Input(shape=(maxlen,), dtype='int32')\n",
    "    article_emb = embedding_layer(article_input)\n",
    "    \n",
    "    article_lstm = LSTM(HIDDEN_DIM, dropout=0.2, recurrent_dropout=0.3)\n",
    "    article_vector = article_lstm(article_emb)\n",
    "    #vector shape: (batch_size, hidden_dim)\n",
    "    \n",
    "    dense_vector = Dense(HIDDEN_DIM*4)(article_vector)\n",
    "    \n",
    "    dense_vector = Dropout(0.3)(dense_vector)\n",
    "    output = Dense(DATASET_CLASS, activation='sigmoid')(dense_vector)\n",
    "    \n",
    "    model = Model(article_input, output)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model \n",
    "\n",
    "model = build_model()\n",
    "\n",
    "callbacks_list = [\n",
    "\tkeras.callbacks.EarlyStopping(\n",
    "\t\tmonitor='val_acc',\n",
    "\t\tpatience=2)\n",
    "]\n",
    "\n",
    "history = model.fit(X_train, Y_train, \n",
    "                    epochs=EPOCHS,\n",
    "                    batch_size=128,\n",
    "                    validation_data=(X_test, Y_test),\n",
    "                    callbacks=callbacks_list)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "#\n",
    "# acc = history.history['acc']\n",
    "# val_acc = history.history['val_acc']\n",
    "# loss = history.history['loss']\n",
    "# val_loss = history.history['val_loss']\n",
    "#\n",
    "# epochs = range(1, len(acc)+1)\n",
    "#\n",
    "# plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
    "# plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
    "# plt.title('Training and validation accracy')\n",
    "# plt.legend()\n",
    "#\n",
    "# plt.figure()\n",
    "#\n",
    "# plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "# plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "# plt.title('Trianing and validation loss')\n",
    "# plt.legend()\n",
    "#\n",
    "# plt.show()\n",
    "#\n",
    "print ('BiLSTM test accuracy: ', model.evaluate(X_test, Y_test))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
