{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from data_reader import *\n",
    "from evaluate_new import *\n",
    "from nltk.tokenize import  SpaceTokenizer\n",
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model, Input\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn import metrics\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.4\n",
    "## prepare data for doc evaluation\n",
    "def get_doc_test(gold, text):\n",
    "    ## gold: gold data\n",
    "    ## text: full text file\n",
    "    test_labels = []\n",
    "    test_doc = []\n",
    "    with open(doc_dir+gold, 'r') as doc_labels, open(doc_dir+text, 'r') as doc_text:\n",
    "        d_labels = doc_labels.readlines()\n",
    "        d_text = doc_text.readlines()\n",
    "        assert len(d_labels) == len(d_text), \"Mismatch\"\n",
    "        for i in range(len(d_labels)):\n",
    "            ## label: start_id end_id data_id pub_id\n",
    "            test_labels.append(d_labels[i].strip())\n",
    "            \n",
    "            text = d_text[i].strip()\n",
    "            text = re.sub('\\d', '0', text)\n",
    "            text = re.sub('[^ ]- ', '', text)\n",
    "            \n",
    "            test_doc.append(text)\n",
    "    return test_labels, test_doc\n",
    "\n",
    "## convert one doc data to (text, label) format\n",
    "def read_doc(doc, labels):\n",
    "    doc = doc.strip().split()\n",
    "    labels = labels.strip().split('|')\n",
    "    labels = [la.split() for la in labels]\n",
    "    for i in range(len(labels)):\n",
    "        for j in range(len(labels[i])):\n",
    "            labels[i][j] = int(labels[i][j])\n",
    "\n",
    "    res_labels = [0]*len(doc)\n",
    "    for la in labels:\n",
    "        if la[2]!=0:\n",
    "            start = la[0]\n",
    "            end = la[1]\n",
    "            res_labels[start : end+1] = [1]*(end+1-start)\n",
    "    return [(doc[i], str(res_labels[i])) for i in range(len(doc))]\n",
    "\n",
    "## make prediction of one doc\n",
    "## split into segments first then combine results\n",
    "def doc_pred(model, doc, tokenizer, MAXLEN=40):\n",
    "    splits = []\n",
    "    for i in range(0, len(doc), MAXLEN):\n",
    "        splits.append(doc[i : i+MAXLEN])\n",
    "    splits = tokenizer.texts_to_sequences(splits)\n",
    "    splits = pad_sequences(splits, maxlen=MAXLEN)\n",
    "    preds = model.predict(splits)\n",
    "    preds = np.squeeze(preds)\n",
    "    preds = [1 if p>=threshold else 0 for pd in preds for p in pd]\n",
    "    return preds\n",
    "\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [int(label) for token, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, label in sent]\n",
    "\n",
    "\n",
    "def prep_data(neg_ratio=0, val_ratio=0.05, data_dir='../../data/data_40/', maxlen=40, emb_dim=300):\n",
    "    train_sents, val_sents = data_sampler(neg_ratio, val_ratio, data_dir)\n",
    "\n",
    "    train_sents = get_sents(train_sents)\n",
    "    val_sents = get_sents(val_sents)\n",
    "\n",
    "    X_train = [sent2tokens(s) for s in train_sents]\n",
    "    Y_train = [sent2labels(s) for s in train_sents]\n",
    "\n",
    "    X_val = [sent2tokens(s) for s in val_sents]\n",
    "    Y_val = [sent2labels(s) for s in val_sents]\n",
    "    \n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "    word_index = tokenizer.word_index\n",
    "\n",
    "    vocab_size = len(word_index)+1\n",
    "    print (\"Vocab size: \", vocab_size)\n",
    "\n",
    "    all_embs = np.stack(embedding_index.values())\n",
    "    emb_mean, emb_std = np.mean(all_embs), np.std(all_embs)\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (vocab_size, emb_dim))\n",
    "    counter = 0\n",
    "    # embedding_matrix = np.zeros((vocab_size, emb_dim))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            counter += 1\n",
    "        else:\n",
    "            embedding_matrix[i] = np.random.randn(emb_dim)\n",
    "    print (\"{}/{} words covered in glove\".format(counter, vocab_size))\n",
    "\n",
    "    X_train = tokenizer.texts_to_sequences(X_train)\n",
    "    X_val = tokenizer.texts_to_sequences(X_val)\n",
    "\n",
    "    X_train = pad_sequences(X_train, maxlen=maxlen)\n",
    "    X_val = pad_sequences(X_val, maxlen=maxlen)\n",
    "\n",
    "    Y_train = np.asarray(Y_train)\n",
    "    Y_val = np.asarray(Y_val)\n",
    "\n",
    "    #labels need to be 3D\n",
    "    Y_train = np.expand_dims(Y_train, axis=2)\n",
    "    Y_val = np.expand_dims(Y_val, axis=2)\n",
    "\n",
    "    return X_train, Y_train, X_val, Y_val, embedding_matrix, vocab_size, tokenizer\n",
    "\n",
    "\n",
    "def run(X_train, Y_train, X_val, Y_val, embedding_matrix, vocab_size, maxlen=40, emb_dim=300, neg_ratio=0, hidden_dim=300, drop=0.2, r_drop=0.1):\n",
    "    ##build model\n",
    "    input = Input(shape=(maxlen,))\n",
    "    model = Embedding(vocab_size, emb_dim, weights=[embedding_matrix], input_length=maxlen, trainable=False)(input)\n",
    "    model = Dropout(drop)(model)\n",
    "    model = Bidirectional(LSTM(hidden_dim, return_sequences=True, recurrent_dropout=r_drop))(model)\n",
    "    model = Dropout(drop)(model)\n",
    "    out = TimeDistributed(Dense(1, activation='sigmoid'))(model)\n",
    "\n",
    "    model = Model(input, out)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "    earlyStop = [EarlyStopping(monitor='val_loss', patience=1)]\n",
    "    history = model.fit(X_train, Y_train, batch_size=64, epochs=10, validation_data=(X_val, Y_val), \n",
    "        callbacks=earlyStop) \n",
    "\n",
    "\n",
    "    pred = model.predict(X_val)\n",
    "    Y_pred = np.squeeze(pred)\n",
    "    test = [[1 if y>=threshold else 0 for y in x] for x in Y_pred]\n",
    "    test_arr = np.asarray(test)\n",
    "    test_arr = np.reshape(test_arr, (-1))\n",
    "    target = np.reshape(Y_val, (-1))\n",
    "\n",
    "    print (metrics.precision_recall_fscore_support(target, test_arr, average=None,\n",
    "                                              labels=[0, 1]))\n",
    "\n",
    "    \n",
    "#     Y_pred_ = [[1 if y>=threshold else 0 for y in x] for x in Y_pred]\n",
    "    Y_val_ = np.squeeze(Y_val)\n",
    "\n",
    "    print (\"Evaluate: dev seg exact\")\n",
    "    pred_out_dir = out_dir+'seg_'+str(neg_ratio)+'neg'\n",
    "    gold_dir = '../../data/val_segs/'+'seg_'+str(neg_ratio)+'neg'\n",
    "    p, r, f = seg_exact_match(test, Y_val_, pred_out_dir, gold_dir)\n",
    "    \n",
    "    return model, history, p, r, f\n",
    "\n",
    "\n",
    "def doc_eval(model, tokenizer, doc_test, doc_out_dir, gold_dir, MAXLEN=40):\n",
    "    doc_preds = [doc_pred(model, d, tokenizer, MAXLEN) for d in doc_test]\n",
    "    doc_preds = [[int(a) for a in x] for x in doc_preds]\n",
    "\n",
    "    with open(doc_out_dir, 'w') as fout:\n",
    "        for i in range(len(doc_preds)):\n",
    "            first = 0\n",
    "            j = 0\n",
    "            string = ''\n",
    "            no_mention = True\n",
    "            while j<len(doc_preds[i]):\n",
    "                while j<len(doc_preds[i]) and doc_preds[i][j]== 0:\n",
    "                    j+=1\n",
    "                if j<len(doc_preds[i]) and doc_preds[i][j] == 1:\n",
    "                    no_mention=False\n",
    "                    start = j\n",
    "                    while j+1<len(doc_preds[i]) and doc_preds[i][j+1]==1:\n",
    "                        j+=1\n",
    "                    end = j \n",
    "                    if first > 0:\n",
    "                        string += \" | \"\n",
    "                    string += (str(start)+' '+str(end))\n",
    "                    j+=1\n",
    "                    first += 1\n",
    "            if no_mention:\n",
    "                fout.write(\"-1 -1\"+'\\n')\n",
    "            else:\n",
    "                fout.write(string+'\\n')\n",
    "    print ('evaluating data from: ', doc_out_dir)\n",
    "    print ('doc exact: ', doc_exact_match(doc_out_dir, gold_dir))\n",
    "    print ('doc partial: ', doc_partial_match(doc_out_dir, gold_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##load glove\n",
    "embedding_index = {}\n",
    "f = open('../../glove.6B.300d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embedding_index[word] = coefs\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_dir = \"../../data/all_test_docs/\"\n",
    "doc_test_y, doc_test_x = get_doc_test('test_doc_gold', 'test_docs')\n",
    "doc_tests = [read_doc(doc_test_x[d], doc_test_y[d]) for d in range(len(doc_test_x))]\n",
    "doc_tests = [sent2tokens(s) for s in doc_tests]\n",
    "\n",
    "zero_shot_y, zero_shot_x = get_doc_test('zero_shot_doc_gold', 'zero_shot_docs')\n",
    "zero_shot_tests = [read_doc(zero_shot_x[d], zero_shot_y[d]) for d in range(len(zero_shot_x))]\n",
    "zero_shot_tests = [sent2tokens(s) for s in zero_shot_tests]\n",
    "\n",
    "MAXLEN = 40\n",
    "DIR = '../../data/data_40/'\n",
    "out_dir = '../../outputs/'\n",
    "\n",
    "if not os.path.exists(out):\n",
    "        os.makedirs(out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_ratio = 0.025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26407 pos data sampled\n",
      "11589 neg data sampled\n",
      "Vocab size:  68498\n",
      "19964/68498 words covered in glove\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train, X_val, Y_val, embedding_matrix, vocab_size, tokenizer = prep_data(neg_ratio=neg_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0neg: 0.637\n",
    "#0.025neg: 68.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36096 samples, validate on 1900 samples\n",
      "Epoch 1/10\n",
      "36096/36096 [==============================] - 369s 10ms/step - loss: 0.0731 - acc: 0.9763 - val_loss: 0.0697 - val_acc: 0.9763\n",
      "Epoch 2/10\n",
      "36096/36096 [==============================] - 375s 10ms/step - loss: 0.0458 - acc: 0.9834 - val_loss: 0.0660 - val_acc: 0.9775\n",
      "Epoch 3/10\n",
      "36096/36096 [==============================] - 390s 11ms/step - loss: 0.0391 - acc: 0.9851 - val_loss: 0.0674 - val_acc: 0.9775\n",
      "(array([0.98915476, 0.69591384]), array([0.98685166, 0.73552059]), array([0.98800186, 0.71516927]), array([73013,  2987]))\n",
      "Evaluate: dev seg exact\n",
      "Doc exact: \n",
      " precision: 0.318686401480111, recall: 0.3626315789473684, f1: 0.3392417528311177\n"
     ]
    }
   ],
   "source": [
    "model, history, p, r, f = run(X_train, Y_train, X_val, Y_val, embedding_matrix, vocab_size, neg_ratio=neg_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating data from:  ../../outputs/doc_40_0.025neg\n",
      "Doc exact: \n",
      " precision: 0.11183673469387755, recall: 0.1454352441613588, f1: 0.12644208583294878\n",
      "doc exact:  (0.11183673469387755, 0.1454352441613588, 0.12644208583294878)\n",
      "Doc Token wise: \n",
      " precision: 0.2749412782791445, recall: 0.35675328841835097, f1: 0.3105494658940166\n",
      "doc partial:  (0.2749412782791445, 0.35675328841835097, 0.3105494658940166)\n",
      "evaluating data from:  ../../outputs/zeroshot_40_0.025neg\n",
      "Doc exact: \n",
      " precision: 0.06515264333581534, recall: 0.07536606373815675, f1: 0.06988817891373801\n",
      "doc exact:  (0.06515264333581534, 0.07536606373815675, 0.06988817891373801)\n",
      "Doc Token wise: \n",
      " precision: 0.23661183059152957, recall: 0.23126924392747178, f1: 0.23391003460207613\n",
      "doc partial:  (0.23661183059152957, 0.23126924392747178, 0.23391003460207613)\n"
     ]
    }
   ],
   "source": [
    "doc_eval(model, tokenizer, doc_tests, out_dir+'doc_40_'+str(neg_ratio)+'neg', '../../data/all_test_docs/test_doc_gold')\n",
    "doc_eval(model, tokenizer, zero_shot_tests, out_dir+'zeroshot_40_'+str(neg_ratio)+'neg', '../../data/all_test_docs/zero_shot_doc_gold')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden dim: 100\n",
      "drop: 0.1\n",
      "Train on 36096 samples, validate on 1900 samples\n",
      "Epoch 1/10\n",
      "36096/36096 [==============================] - 134s 4ms/step - loss: 0.0819 - acc: 0.9738 - val_loss: 0.0716 - val_acc: 0.9762\n",
      "Epoch 2/10\n",
      "36096/36096 [==============================] - 106s 3ms/step - loss: 0.0488 - acc: 0.9827 - val_loss: 0.0665 - val_acc: 0.9772\n",
      "Epoch 3/10\n",
      "36096/36096 [==============================] - 105s 3ms/step - loss: 0.0417 - acc: 0.9844 - val_loss: 0.0642 - val_acc: 0.9779\n",
      "Epoch 4/10\n",
      "36096/36096 [==============================] - 105s 3ms/step - loss: 0.0376 - acc: 0.9854 - val_loss: 0.0633 - val_acc: 0.9778\n",
      "Epoch 5/10\n",
      "36096/36096 [==============================] - 105s 3ms/step - loss: 0.0351 - acc: 0.9861 - val_loss: 0.0660 - val_acc: 0.9775\n",
      "(array([0.98952836, 0.68842822]), array([0.98620794, 0.74489454]), array([0.98786536, 0.71554912]), array([73013,  2987]))\n",
      "Evaluate: dev seg exact\n",
      "Doc exact: \n",
      " precision: 0.31934731934731936, recall: 0.3605263157894737, f1: 0.33868974042027195\n",
      "evaluating data from:  ../../outputs/doc_40_0.025neg\n",
      "Doc exact: \n",
      " precision: 0.11833658945018141, recall: 0.15003538570417552, f1: 0.13231393353097204\n",
      "doc exact:  (0.11833658945018141, 0.15003538570417552, 0.13231393353097204)\n",
      "Doc Token wise: \n",
      " precision: 0.2958212113197567, recall: 0.35883862688482515, f1: 0.32429689765149317\n",
      "doc partial:  (0.2958212113197567, 0.35883862688482515, 0.32429689765149317)\n",
      "evaluating data from:  ../../outputs/zeroshot_40_0.025neg\n",
      "Doc exact: \n",
      " precision: 0.06567717996289425, recall: 0.07622739018087855, f1: 0.070560095674706\n",
      "doc exact:  (0.06567717996289425, 0.07622739018087855, 0.070560095674706)\n",
      "Doc Token wise: \n",
      " precision: 0.2547035853745119, recall: 0.2454669859733151, f1: 0.25\n",
      "doc partial:  (0.2547035853745119, 0.2454669859733151, 0.25)\n",
      "hidden dim: 100\n",
      "drop: 0.2\n",
      "Train on 36096 samples, validate on 1900 samples\n",
      "Epoch 1/10\n",
      "36096/36096 [==============================] - 114s 3ms/step - loss: 0.0845 - acc: 0.9735 - val_loss: 0.0735 - val_acc: 0.9757\n",
      "Epoch 2/10\n",
      "36096/36096 [==============================] - 109s 3ms/step - loss: 0.0523 - acc: 0.9818 - val_loss: 0.0686 - val_acc: 0.9759\n",
      "Epoch 3/10\n",
      "36096/36096 [==============================] - 108s 3ms/step - loss: 0.0447 - acc: 0.9837 - val_loss: 0.0668 - val_acc: 0.9769\n",
      "Epoch 4/10\n",
      "36096/36096 [==============================] - 108s 3ms/step - loss: 0.0408 - acc: 0.9846 - val_loss: 0.0665 - val_acc: 0.9774\n",
      "Epoch 5/10\n",
      "36096/36096 [==============================] - 109s 3ms/step - loss: 0.0380 - acc: 0.9853 - val_loss: 0.0658 - val_acc: 0.9776\n",
      "Epoch 6/10\n",
      "36096/36096 [==============================] - 108s 3ms/step - loss: 0.0362 - acc: 0.9857 - val_loss: 0.0687 - val_acc: 0.9768\n",
      "(array([0.99095434, 0.67002012]), array([0.98427677, 0.78038165]), array([0.98760427, 0.72100217]), array([73013,  2987]))\n",
      "Evaluate: dev seg exact\n",
      "Doc exact: \n",
      " precision: 0.32570905763952424, recall: 0.37473684210526315, f1: 0.34850709740577585\n",
      "evaluating data from:  ../../outputs/doc_40_0.025neg\n",
      "Doc exact: \n",
      " precision: 0.07950028392958547, recall: 0.14861995753715498, f1: 0.10358860525342213\n",
      "doc exact:  (0.07950028392958547, 0.14861995753715498, 0.10358860525342213)\n",
      "Doc Token wise: \n",
      " precision: 0.21205474644674505, recall: 0.3877125441129291, f1: 0.2741606170598911\n",
      "doc partial:  (0.21205474644674505, 0.3877125441129291, 0.2741606170598911)\n",
      "evaluating data from:  ../../outputs/zeroshot_40_0.025neg\n",
      "Doc exact: \n",
      " precision: 0.03923154701718908, recall: 0.08354866494401378, f1: 0.053392046236411174\n",
      "doc exact:  (0.03923154701718908, 0.08354866494401378, 0.053392046236411174)\n",
      "Doc Token wise: \n",
      " precision: 0.16362393829932637, recall: 0.28669175504618544, f1: 0.20834110261669464\n",
      "doc partial:  (0.16362393829932637, 0.28669175504618544, 0.20834110261669464)\n",
      "hidden dim: 100\n",
      "drop: 0.3\n",
      "Train on 36096 samples, validate on 1900 samples\n",
      "Epoch 1/10\n",
      "36096/36096 [==============================] - 115s 3ms/step - loss: 0.0843 - acc: 0.9738 - val_loss: 0.0741 - val_acc: 0.9752\n",
      "Epoch 2/10\n",
      "36096/36096 [==============================] - 109s 3ms/step - loss: 0.0540 - acc: 0.9813 - val_loss: 0.0683 - val_acc: 0.9764\n",
      "Epoch 3/10\n",
      "36096/36096 [==============================] - 115s 3ms/step - loss: 0.0469 - acc: 0.9831 - val_loss: 0.0719 - val_acc: 0.9757\n",
      "(array([0.99055914, 0.64891519]), array([0.98293455, 0.7710077 ]), array([0.98673211, 0.70471236]), array([73013,  2987]))\n",
      "Evaluate: dev seg exact\n",
      "Doc exact: \n",
      " precision: 0.30961968680089486, recall: 0.3642105263157895, f1: 0.3347037484885127\n",
      "evaluating data from:  ../../outputs/doc_40_0.025neg\n",
      "Doc exact: \n",
      " precision: 0.06674592391304347, recall: 0.13906581740976645, f1: 0.09019967867798942\n",
      "doc exact:  (0.06674592391304347, 0.13906581740976645, 0.09019967867798942)\n",
      "Doc Token wise: \n",
      " precision: 0.19100861868157465, recall: 0.3946102021174206, f1: 0.2574164181447183\n",
      "doc partial:  (0.19100861868157465, 0.3946102021174206, 0.2574164181447183)\n",
      "evaluating data from:  ../../outputs/zeroshot_40_0.025neg\n",
      "Doc exact: \n",
      " precision: 0.03206302674972517, recall: 0.07536606373815675, f1: 0.044987146529562975\n",
      "doc exact:  (0.03206302674972517, 0.07536606373815675, 0.044987146529562975)\n",
      "Doc Token wise: \n",
      " precision: 0.14546561168594035, recall: 0.2861785836469381, f1: 0.19288637804807748\n",
      "doc partial:  (0.14546561168594035, 0.2861785836469381, 0.19288637804807748)\n",
      "hidden dim: 200\n",
      "drop: 0.1\n",
      "Train on 36096 samples, validate on 1900 samples\n",
      "Epoch 1/10\n",
      "36096/36096 [==============================] - 223s 6ms/step - loss: 0.0750 - acc: 0.9759 - val_loss: 0.0698 - val_acc: 0.9761\n",
      "Epoch 2/10\n",
      "36096/36096 [==============================] - 215s 6ms/step - loss: 0.0463 - acc: 0.9833 - val_loss: 0.0655 - val_acc: 0.9769\n",
      "Epoch 3/10\n",
      "36096/36096 [==============================] - 216s 6ms/step - loss: 0.0393 - acc: 0.9850 - val_loss: 0.0647 - val_acc: 0.9777\n",
      "Epoch 4/10\n",
      "36096/36096 [==============================] - 216s 6ms/step - loss: 0.0354 - acc: 0.9859 - val_loss: 0.0654 - val_acc: 0.9771\n",
      "(array([0.98963517, 0.68623233]), array([0.98601619, 0.74757282]), array([0.98782236, 0.71559045]), array([73013,  2987]))\n",
      "Evaluate: dev seg exact\n",
      "Doc exact: \n",
      " precision: 0.31095571095571095, recall: 0.3510526315789474, f1: 0.32978986402966626\n",
      "evaluating data from:  ../../outputs/doc_40_0.025neg\n",
      "Doc exact: \n",
      " precision: 0.09078860172299535, recall: 0.1454352441613588, f1: 0.11179110567115462\n",
      "doc exact:  (0.09078860172299535, 0.1454352441613588, 0.11179110567115462)\n",
      "Doc Token wise: \n",
      " precision: 0.23440491786758036, recall: 0.37311517484760987, f1: 0.287924738503435\n",
      "doc partial:  (0.23440491786758036, 0.37311517484760987, 0.287924738503435)\n",
      "evaluating data from:  ../../outputs/zeroshot_40_0.025neg\n",
      "Doc exact: \n",
      " precision: 0.04759506421556283, recall: 0.08139534883720931, f1: 0.060066740823136816\n",
      "doc exact:  (0.04759506421556283, 0.08139534883720931, 0.060066740823136816)\n",
      "Doc Token wise: \n",
      " precision: 0.18734476641040804, recall: 0.2709544988026001, f1: 0.22152297042164884\n",
      "doc partial:  (0.18734476641040804, 0.2709544988026001, 0.22152297042164884)\n",
      "hidden dim: 200\n",
      "drop: 0.2\n",
      "Train on 36096 samples, validate on 1900 samples\n",
      "Epoch 1/10\n",
      "36096/36096 [==============================] - 228s 6ms/step - loss: 0.0785 - acc: 0.9748 - val_loss: 0.0706 - val_acc: 0.9761\n",
      "Epoch 2/10\n",
      "36096/36096 [==============================] - 218s 6ms/step - loss: 0.0489 - acc: 0.9827 - val_loss: 0.0667 - val_acc: 0.9766\n",
      "Epoch 3/10\n",
      "36096/36096 [==============================] - 216s 6ms/step - loss: 0.0420 - acc: 0.9843 - val_loss: 0.0685 - val_acc: 0.9776\n",
      "(array([0.98880674, 0.70054856]), array([0.98728993, 0.7268162 ]), array([0.98804775, 0.71344068]), array([73013,  2987]))\n",
      "Evaluate: dev seg exact\n",
      "Doc exact: \n",
      " precision: 0.3085955847815876, recall: 0.34578947368421054, f1: 0.32613551749813846\n",
      "evaluating data from:  ../../outputs/doc_40_0.025neg\n",
      "Doc exact: \n",
      " precision: 0.10068931560807484, recall: 0.14472753007784855, f1: 0.11875725900116144\n",
      "doc exact:  (0.10068931560807484, 0.14472753007784855, 0.11875725900116144)\n",
      "Doc Token wise: \n",
      " precision: 0.26099339257233994, recall: 0.36750080205325636, f1: 0.3052224886757261\n",
      "doc partial:  (0.26099339257233994, 0.36750080205325636, 0.3052224886757261)\n",
      "evaluating data from:  ../../outputs/zeroshot_40_0.025neg\n",
      "Doc exact: \n",
      " precision: 0.05130513051305131, recall: 0.07364341085271318, f1: 0.060477453580901855\n",
      "doc exact:  (0.05130513051305131, 0.07364341085271318, 0.060477453580901855)\n",
      "Doc Token wise: \n",
      " precision: 0.2073490813648294, recall: 0.24324324324324326, f1: 0.2238664987405542\n",
      "doc partial:  (0.2073490813648294, 0.24324324324324326, 0.2238664987405542)\n",
      "hidden dim: 200\n",
      "drop: 0.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36096 samples, validate on 1900 samples\n",
      "Epoch 1/10\n",
      "36096/36096 [==============================] - 222s 6ms/step - loss: 0.0794 - acc: 0.9746 - val_loss: 0.0731 - val_acc: 0.9754\n",
      "Epoch 2/10\n",
      "36096/36096 [==============================] - 214s 6ms/step - loss: 0.0510 - acc: 0.9821 - val_loss: 0.0697 - val_acc: 0.9764\n",
      "Epoch 3/10\n",
      "36096/36096 [==============================] - 211s 6ms/step - loss: 0.0439 - acc: 0.9839 - val_loss: 0.0661 - val_acc: 0.9772\n",
      "Epoch 4/10\n",
      "36096/36096 [==============================] - 208s 6ms/step - loss: 0.0397 - acc: 0.9848 - val_loss: 0.0676 - val_acc: 0.9774\n",
      "(array([0.98977921, 0.67897126]), array([0.98546834, 0.75125544]), array([0.98761907, 0.71328671]), array([73013,  2987]))\n",
      "Evaluate: dev seg exact\n",
      "Doc exact: \n",
      " precision: 0.3185799907791609, recall: 0.3636842105263158, f1: 0.339641189481445\n",
      "evaluating data from:  ../../outputs/doc_40_0.025neg\n",
      "Doc exact: \n",
      " precision: 0.09102960671674767, recall: 0.14578910120311395, f1: 0.11207834602829163\n",
      "doc exact:  (0.09102960671674767, 0.14578910120311395, 0.11207834602829163)\n",
      "Doc Token wise: \n",
      " precision: 0.2360191504532953, recall: 0.3716714789862047, f1: 0.2887047535979067\n",
      "doc partial:  (0.2360191504532953, 0.3716714789862047, 0.2887047535979067)\n",
      "evaluating data from:  ../../outputs/zeroshot_40_0.025neg\n",
      "Doc exact: \n",
      " precision: 0.04631578947368421, recall: 0.07579672695951765, f1: 0.05749754982032016\n",
      "doc exact:  (0.04631578947368421, 0.07579672695951765, 0.05749754982032016)\n",
      "Doc Token wise: \n",
      " precision: 0.189389265885256, recall: 0.26257269928156, f1: 0.22005590997061142\n",
      "doc partial:  (0.189389265885256, 0.26257269928156, 0.22005590997061142)\n",
      "hidden dim: 300\n",
      "drop: 0.1\n",
      "Train on 36096 samples, validate on 1900 samples\n",
      "Epoch 1/10\n",
      "36096/36096 [==============================] - 356s 10ms/step - loss: 0.0747 - acc: 0.9760 - val_loss: 0.0715 - val_acc: 0.9762\n",
      "Epoch 2/10\n",
      "36096/36096 [==============================] - 346s 10ms/step - loss: 0.0459 - acc: 0.9834 - val_loss: 0.0656 - val_acc: 0.9774\n",
      "Epoch 3/10\n",
      "36096/36096 [==============================] - 346s 10ms/step - loss: 0.0384 - acc: 0.9851 - val_loss: 0.0654 - val_acc: 0.9772\n",
      "Epoch 4/10\n",
      "36096/36096 [==============================] - 345s 10ms/step - loss: 0.0345 - acc: 0.9861 - val_loss: 0.0670 - val_acc: 0.9770\n",
      "(array([0.98980981, 0.68413037]), array([0.98579705, 0.75192501]), array([0.98779935, 0.71642743]), array([73013,  2987]))\n",
      "Evaluate: dev seg exact\n",
      "Doc exact: \n",
      " precision: 0.3234338747099768, recall: 0.3668421052631579, f1: 0.3437731196054254\n",
      "evaluating data from:  ../../outputs/doc_40_0.025neg\n",
      "Doc exact: \n",
      " precision: 0.10007486897928625, recall: 0.1418966737438075, f1: 0.11737157910141957\n",
      "doc exact:  (0.10007486897928625, 0.1418966737438075, 0.11737157910141957)\n",
      "Doc Token wise: \n",
      " precision: 0.25871791920819426, recall: 0.3606031440487648, f1: 0.3012799035046572\n",
      "doc partial:  (0.25871791920819426, 0.3606031440487648, 0.3012799035046572)\n",
      "evaluating data from:  ../../outputs/zeroshot_40_0.025neg\n",
      "Doc exact: \n",
      " precision: 0.05412130637636081, recall: 0.07493540051679587, f1: 0.06284991872855336\n",
      "doc exact:  (0.05412130637636081, 0.07493540051679587, 0.06284991872855336)\n",
      "Doc Token wise: \n",
      " precision: 0.21833802399644497, recall: 0.2521382141635306, f1: 0.23402397396205446\n",
      "doc partial:  (0.21833802399644497, 0.2521382141635306, 0.23402397396205446)\n",
      "hidden dim: 300\n",
      "drop: 0.2\n",
      "Train on 36096 samples, validate on 1900 samples\n",
      "Epoch 1/10\n",
      "36096/36096 [==============================] - 357s 10ms/step - loss: 0.0746 - acc: 0.9758 - val_loss: 0.0724 - val_acc: 0.9758\n",
      "Epoch 2/10\n",
      "36096/36096 [==============================] - 346s 10ms/step - loss: 0.0472 - acc: 0.9829 - val_loss: 0.0658 - val_acc: 0.9771\n",
      "Epoch 3/10\n",
      "36096/36096 [==============================] - 345s 10ms/step - loss: 0.0403 - acc: 0.9847 - val_loss: 0.0657 - val_acc: 0.9776\n",
      "Epoch 4/10\n",
      "36096/36096 [==============================] - 346s 10ms/step - loss: 0.0362 - acc: 0.9857 - val_loss: 0.0643 - val_acc: 0.9777\n",
      "Epoch 5/10\n",
      "36096/36096 [==============================] - 345s 10ms/step - loss: 0.0335 - acc: 0.9864 - val_loss: 0.0679 - val_acc: 0.9770\n",
      "(array([0.98867034, 0.69844861]), array([0.98722145, 0.72346836]), array([0.98794537, 0.71073837]), array([73013,  2987]))\n",
      "Evaluate: dev seg exact\n",
      "Doc exact: \n",
      " precision: 0.31179775280898875, recall: 0.3505263157894737, f1: 0.33002973240832506\n",
      "evaluating data from:  ../../outputs/doc_40_0.025neg\n",
      "Doc exact: \n",
      " precision: 0.10729499467518637, recall: 0.14260438782731777, f1: 0.1224551807961106\n",
      "doc exact:  (0.10729499467518637, 0.14260438782731777, 0.1224551807961106)\n",
      "Doc Token wise: \n",
      " precision: 0.28045977011494255, recall: 0.35226179018286813, f1: 0.3122866894197952\n",
      "doc partial:  (0.28045977011494255, 0.35226179018286813, 0.3122866894197952)\n",
      "evaluating data from:  ../../outputs/zeroshot_40_0.025neg\n",
      "Doc exact: \n",
      " precision: 0.060756972111553786, recall: 0.07881136950904392, f1: 0.0686164229471316\n",
      "doc exact:  (0.060756972111553786, 0.07881136950904392, 0.0686164229471316)\n",
      "Doc Token wise: \n",
      " precision: 0.22670757306636524, recall: 0.24016421484775916, f1: 0.23324196361824068\n",
      "doc partial:  (0.22670757306636524, 0.24016421484775916, 0.23324196361824068)\n",
      "hidden dim: 300\n",
      "drop: 0.3\n",
      "Train on 36096 samples, validate on 1900 samples\n",
      "Epoch 1/10\n",
      "36096/36096 [==============================] - 355s 10ms/step - loss: 0.0768 - acc: 0.9753 - val_loss: 0.0733 - val_acc: 0.9761\n",
      "Epoch 2/10\n",
      "36096/36096 [==============================] - 348s 10ms/step - loss: 0.0494 - acc: 0.9824 - val_loss: 0.0684 - val_acc: 0.9766\n",
      "Epoch 3/10\n",
      "36096/36096 [==============================] - 346s 10ms/step - loss: 0.0427 - acc: 0.9842 - val_loss: 0.0669 - val_acc: 0.9770\n",
      "Epoch 4/10\n",
      "36096/36096 [==============================] - 347s 10ms/step - loss: 0.0385 - acc: 0.9851 - val_loss: 0.0680 - val_acc: 0.9774\n",
      "(array([0.99018785, 0.68185907]), array([0.98546834, 0.76129896]), array([0.98782246, 0.7193926 ]), array([73013,  2987]))\n",
      "Evaluate: dev seg exact\n",
      "Doc exact: \n",
      " precision: 0.322253000923361, recall: 0.36736842105263157, f1: 0.3433349729463846\n",
      "evaluating data from:  ../../outputs/doc_40_0.025neg\n",
      "Doc exact: \n",
      " precision: 0.09421449805625429, recall: 0.14578910120311395, f1: 0.11446034171412696\n",
      "doc exact:  (0.09421449805625429, 0.14578910120311395, 0.11446034171412696)\n",
      "Doc Token wise: \n",
      " precision: 0.23890258939580764, recall: 0.37295476419634266, f1: 0.29124389327320555\n",
      "doc partial:  (0.23890258939580764, 0.37295476419634266, 0.29124389327320555)\n",
      "evaluating data from:  ../../outputs/zeroshot_40_0.025neg\n",
      "Doc exact: \n",
      " precision: 0.050149091894822445, recall: 0.07967269595176572, f1: 0.06155381800033273\n",
      "doc exact:  (0.050149091894822445, 0.07967269595176572, 0.06155381800033273)\n",
      "Doc Token wise: \n",
      " precision: 0.20064086763618438, recall: 0.27848101265822783, f1: 0.23323782234957022\n",
      "doc partial:  (0.20064086763618438, 0.27848101265822783, 0.23323782234957022)\n"
     ]
    }
   ],
   "source": [
    "for hidden_dim in [100, 200, 300]:\n",
    "    for drop in [0.1, 0.2, 0.3]:\n",
    "        print ('hidden dim:', hidden_dim)\n",
    "        print ('drop:', drop)\n",
    "        model, history, p, r, f = run(X_train, Y_train, X_val, Y_val, embedding_matrix, vocab_size, neg_ratio=neg_ratio, hidden_dim=hidden_dim, drop=drop)\n",
    "        doc_eval(model, tokenizer, doc_tests, out_dir+'doc_40_'+str(neg_ratio)+'neg', '../../data/all_test_docs/test_doc_gold')\n",
    "        doc_eval(model, tokenizer, zero_shot_tests, out_dir+'zeroshot_40_'+str(neg_ratio)+'neg', '../../data/all_test_docs/zero_shot_doc_gold')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use hidden dim 100, dropout 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36096 samples, validate on 1900 samples\n",
      "Epoch 1/10\n",
      "36096/36096 [==============================] - 72s 2ms/step - loss: 0.0843 - acc: 0.9739 - val_loss: 0.0723 - val_acc: 0.9761\n",
      "Epoch 2/10\n",
      "36096/36096 [==============================] - 64s 2ms/step - loss: 0.0510 - acc: 0.9821 - val_loss: 0.0662 - val_acc: 0.9767\n",
      "Epoch 3/10\n",
      "36096/36096 [==============================] - 64s 2ms/step - loss: 0.0435 - acc: 0.9840 - val_loss: 0.0650 - val_acc: 0.9773\n",
      "Epoch 4/10\n",
      "36096/36096 [==============================] - 66s 2ms/step - loss: 0.0394 - acc: 0.9849 - val_loss: 0.0661 - val_acc: 0.9775\n",
      "(array([0.98926208, 0.69470699]), array([0.98672839, 0.73819886]), array([0.98799361, 0.71579289]), array([73013,  2987]))\n",
      "Evaluate: dev seg exact\n",
      "Doc exact: \n",
      " precision: 0.31529850746268656, recall: 0.35578947368421054, f1: 0.33432245301681507\n",
      "evaluating data from:  ../../outputs/doc_40_0.025neg\n",
      "Doc exact: \n",
      " precision: 0.09567976568220649, recall: 0.1387119603680113, f1: 0.11324570273003032\n",
      "doc exact:  (0.09567976568220649, 0.1387119603680113, 0.11324570273003032)\n",
      "Doc Token wise: \n",
      " precision: 0.256416054267948, recall: 0.3638113570741097, f1: 0.3008157039591485\n",
      "doc partial:  (0.256416054267948, 0.3638113570741097, 0.3008157039591485)\n",
      "evaluating data from:  ../../outputs/zeroshot_40_0.025neg\n",
      "Doc exact: \n",
      " precision: 0.055522740696987594, recall: 0.08096468561584841, f1: 0.06587245970567625\n",
      "doc exact:  (0.055522740696987594, 0.08096468561584841, 0.06587245970567625)\n",
      "Doc Token wise: \n",
      " precision: 0.19975237309120925, recall: 0.24837495723571673, f1: 0.22142584826534503\n",
      "doc partial:  (0.19975237309120925, 0.24837495723571673, 0.22142584826534503)\n"
     ]
    }
   ],
   "source": [
    "hidden_dim = 50\n",
    "drop = 0.1\n",
    "r_drop=0.0\n",
    "model, history, p, r, f = run(X_train, Y_train, X_val, Y_val, embedding_matrix, vocab_size, neg_ratio=neg_ratio, hidden_dim=hidden_dim, drop=drop, r_drop=r_drop)\n",
    "doc_eval(model, tokenizer, doc_tests, out_dir+'doc_40_'+str(neg_ratio)+'neg', '../../data/all_test_docs/test_doc_gold')\n",
    "doc_eval(model, tokenizer, zero_shot_tests, out_dir+'zeroshot_40_'+str(neg_ratio)+'neg', '../../data/all_test_docs/zero_shot_doc_gold')\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36096 samples, validate on 1900 samples\n",
      "Epoch 1/10\n",
      "36096/36096 [==============================] - 112s 3ms/step - loss: 0.0821 - acc: 0.9740 - val_loss: 0.0733 - val_acc: 0.9756\n",
      "Epoch 2/10\n",
      "36096/36096 [==============================] - 99s 3ms/step - loss: 0.0492 - acc: 0.9825 - val_loss: 0.0680 - val_acc: 0.9771\n",
      "Epoch 3/10\n",
      "36096/36096 [==============================] - 107s 3ms/step - loss: 0.0417 - acc: 0.9844 - val_loss: 0.0656 - val_acc: 0.9775\n",
      "Epoch 4/10\n",
      "36096/36096 [==============================] - 97s 3ms/step - loss: 0.0375 - acc: 0.9855 - val_loss: 0.0669 - val_acc: 0.9769\n",
      "(array([0.9898223 , 0.68256379]), array([0.98568748, 0.75225979]), array([0.98775056, 0.71571906]), array([73013,  2987]))\n",
      "Evaluate: dev seg exact\n",
      "Doc exact: \n",
      " precision: 0.3219871205151794, recall: 0.3684210526315789, f1: 0.34364261168384874\n",
      "evaluating data from:  ../../outputs/doc_40_0.025neg\n",
      "Doc exact: \n",
      " precision: 0.09178321678321678, recall: 0.14861995753715498, f1: 0.11348284247500674\n",
      "doc exact:  (0.09178321678321678, 0.14861995753715498, 0.11348284247500674)\n",
      "Doc Token wise: \n",
      " precision: 0.23396150761828388, recall: 0.37439846005774785, f1: 0.2879703886489821\n",
      "doc partial:  (0.23396150761828388, 0.37439846005774785, 0.2879703886489821)\n",
      "evaluating data from:  ../../outputs/zeroshot_40_0.025neg\n",
      "Doc exact: \n",
      " precision: 0.04813664596273292, recall: 0.08010335917312661, f1: 0.060135790494665366\n",
      "doc exact:  (0.04813664596273292, 0.08010335917312661, 0.060135790494665366)\n",
      "Doc Token wise: \n",
      " precision: 0.18979616746002687, recall: 0.265993841943209, f1: 0.2215257496972719\n",
      "doc partial:  (0.18979616746002687, 0.265993841943209, 0.2215257496972719)\n"
     ]
    }
   ],
   "source": [
    "hidden_dim = 100\n",
    "drop = 0.1\n",
    "r_drop=0.0\n",
    "model, history, p, r, f = run(X_train, Y_train, X_val, Y_val, embedding_matrix, vocab_size, neg_ratio=neg_ratio, hidden_dim=hidden_dim, drop=drop, r_drop=r_drop)\n",
    "doc_eval(model, tokenizer, doc_tests, out_dir+'doc_40_'+str(neg_ratio)+'neg', '../../data/all_test_docs/test_doc_gold')\n",
    "doc_eval(model, tokenizer, zero_shot_tests, out_dir+'zeroshot_40_'+str(neg_ratio)+'neg', '../../data/all_test_docs/zero_shot_doc_gold')\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36096 samples, validate on 1900 samples\n",
      "Epoch 1/10\n",
      "36096/36096 [==============================] - 106s 3ms/step - loss: 0.0788 - acc: 0.9750 - val_loss: 0.0715 - val_acc: 0.9761\n",
      "Epoch 2/10\n",
      "36096/36096 [==============================] - 97s 3ms/step - loss: 0.0478 - acc: 0.9829 - val_loss: 0.0655 - val_acc: 0.9775\n",
      "Epoch 3/10\n",
      "36096/36096 [==============================] - 97s 3ms/step - loss: 0.0407 - acc: 0.9847 - val_loss: 0.0654 - val_acc: 0.9778\n",
      "Epoch 4/10\n",
      "36096/36096 [==============================] - 97s 3ms/step - loss: 0.0368 - acc: 0.9856 - val_loss: 0.0649 - val_acc: 0.9779\n",
      "Epoch 5/10\n",
      "36096/36096 [==============================] - 97s 3ms/step - loss: 0.0341 - acc: 0.9862 - val_loss: 0.0673 - val_acc: 0.9776\n",
      "(array([0.98729005, 0.72711205]), array([0.98942654, 0.68865082]), array([0.98835714, 0.70735901]), array([73013,  2987]))\n",
      "Evaluate: dev seg exact\n",
      "Doc exact: \n",
      " precision: 0.2980392156862745, recall: 0.32, f1: 0.3086294416243655\n",
      "evaluating data from:  ../../outputs/doc_40_0.025neg\n",
      "Doc exact: \n",
      " precision: 0.11169284467713787, recall: 0.13588110403397027, f1: 0.12260536398467432\n",
      "doc exact:  (0.11169284467713787, 0.13588110403397027, 0.12260536398467432)\n",
      "Doc Token wise: \n",
      " precision: 0.3036953136061164, recall: 0.3440808469682387, f1: 0.3226291644731895\n",
      "doc partial:  (0.3036953136061164, 0.3440808469682387, 0.3226291644731895)\n",
      "evaluating data from:  ../../outputs/zeroshot_40_0.025neg\n",
      "Doc exact: \n",
      " precision: 0.06393380970289582, recall: 0.07321274763135228, f1: 0.068259385665529\n",
      "doc exact:  (0.06393380970289582, 0.07321274763135228, 0.068259385665529)\n",
      "Doc Token wise: \n",
      " precision: 0.23860123647604328, recall: 0.2112555593568252, f1: 0.22409726002540376\n",
      "doc partial:  (0.23860123647604328, 0.2112555593568252, 0.22409726002540376)\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.5\n",
    "hidden_dim = 100\n",
    "drop = 0.1\n",
    "r_drop=0.0\n",
    "model, history, p, r, f = run(X_train, Y_train, X_val, Y_val, embedding_matrix, vocab_size, neg_ratio=neg_ratio, hidden_dim=hidden_dim, drop=drop, r_drop=r_drop)\n",
    "doc_eval(model, tokenizer, doc_tests, out_dir+'doc_40_'+str(neg_ratio)+'neg', '../../data/all_test_docs/test_doc_gold')\n",
    "doc_eval(model, tokenizer, zero_shot_tests, out_dir+'zeroshot_40_'+str(neg_ratio)+'neg', '../../data/all_test_docs/zero_shot_doc_gold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36096 samples, validate on 1900 samples\n",
      "Epoch 1/10\n",
      "36096/36096 [==============================] - 89s 2ms/step - loss: 0.0857 - acc: 0.9729 - val_loss: 0.0729 - val_acc: 0.9758\n",
      "Epoch 2/10\n",
      "36096/36096 [==============================] - 80s 2ms/step - loss: 0.0505 - acc: 0.9822 - val_loss: 0.0674 - val_acc: 0.9768\n",
      "Epoch 3/10\n",
      "36096/36096 [==============================] - 80s 2ms/step - loss: 0.0429 - acc: 0.9841 - val_loss: 0.0654 - val_acc: 0.9776\n",
      "Epoch 4/10\n",
      "36096/36096 [==============================] - 80s 2ms/step - loss: 0.0388 - acc: 0.9851 - val_loss: 0.0654 - val_acc: 0.9773\n",
      "Epoch 5/10\n",
      "36096/36096 [==============================] - 80s 2ms/step - loss: 0.0360 - acc: 0.9858 - val_loss: 0.0668 - val_acc: 0.9774\n",
      "(array([0.98707474, 0.72633452]), array([0.98946763, 0.68329428]), array([0.98826973, 0.70415732]), array([73013,  2987]))\n",
      "Evaluate: dev seg exact\n",
      "Doc exact: \n",
      " precision: 0.3002008032128514, recall: 0.31473684210526315, f1: 0.30729701952723537\n",
      "evaluating data from:  ../../outputs/doc_40_0.025neg\n",
      "Doc exact: \n",
      " precision: 0.11581920903954802, recall: 0.1305732484076433, f1: 0.12275449101796407\n",
      "doc exact:  (0.11581920903954802, 0.1305732484076433, 0.12275449101796407)\n",
      "Doc Token wise: \n",
      " precision: 0.2966831333803811, recall: 0.3371831889637472, f1: 0.31563931226068026\n",
      "doc partial:  (0.2966831333803811, 0.3371831889637472, 0.31563931226068026)\n",
      "evaluating data from:  ../../outputs/zeroshot_40_0.025neg\n",
      "Doc exact: \n",
      " precision: 0.06412478336221837, recall: 0.06373815676141258, f1: 0.06393088552915767\n",
      "doc exact:  (0.06412478336221837, 0.06373815676141258, 0.06393088552915767)\n",
      "Doc Token wise: \n",
      " precision: 0.2501510574018127, recall: 0.21245295928840233, f1: 0.22976597909536586\n",
      "doc partial:  (0.2501510574018127, 0.21245295928840233, 0.22976597909536586)\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.5\n",
    "hidden_dim = 80\n",
    "drop = 0.1\n",
    "r_drop=0.0\n",
    "model, history, p, r, f = run(X_train, Y_train, X_val, Y_val, embedding_matrix, vocab_size, neg_ratio=neg_ratio, hidden_dim=hidden_dim, drop=drop, r_drop=r_drop)\n",
    "doc_eval(model, tokenizer, doc_tests, out_dir+'doc_40_'+str(neg_ratio)+'neg', '../../data/all_test_docs/test_doc_gold')\n",
    "doc_eval(model, tokenizer, zero_shot_tests, out_dir+'zeroshot_40_'+str(neg_ratio)+'neg', '../../data/all_test_docs/zero_shot_doc_gold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold = 0.5\n",
    "hidden_dim = 300\n",
    "drop = 0.3\n",
    "r_drop=0.1\n",
    "model, history, p, r, f = run(X_train, Y_train, X_val, Y_val, embedding_matrix, vocab_size, neg_ratio=neg_ratio, hidden_dim=hidden_dim, drop=drop, r_drop=r_drop)\n",
    "doc_eval(model, tokenizer, doc_tests, out_dir+'doc_40_'+str(neg_ratio)+'neg', '../../data/all_test_docs/test_doc_gold')\n",
    "doc_eval(model, tokenizer, zero_shot_tests, out_dir+'zeroshot_40_'+str(neg_ratio)+'neg', '../../data/all_test_docs/zero_shot_doc_gold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
